import os
from config import (
    AIHUB_FILE_KEYS,
    DOWNLOAD_DIR,
    EXTRACT_DIR,
    TRANSFORM_SRC_DIR,
    TRANSFORM_DST_DIR,
    BUCKET_NAME
)
from utils.logger import pipeline_logger
from aihub_downloader import download_file
from extractor import unzip_file
from image_transformer import run_transform_for_chunk
from archiver import compress_folder
from bucket_uploader import upload_to_s3
from cleanup_manager import cleanup_chunk_files
from utils.utils import check_storage

def run_pipeline():
    pipeline_logger.info("======================================================")
    pipeline_logger.info("ğŸš€ AIHub MLOps íŒŒì´í”„ë¼ì¸ ì‹œì‘ (ì¤‘ì•™ ì œì–´)")
    pipeline_logger.info("======================================================")


    # 1. íŒŒì¼ë³„ ë‹¤ìš´ë¡œë“œ -> ì••ì¶• í•´ì œ -> ë³€í™˜ -> ì ì¬ -> ì •ë¦¬ íŠ¸ëœì­ì…˜ ë£¨í”„
    for key in AIHUB_FILE_KEYS:
        pipeline_logger.info(f"\n--- [ {key} íŠ¸ëœì­ì…˜ ì‹œì‘ ] ---")
        
        # [Step 1] ë‹¤ìš´ë¡œë“œ ì‹œë„ (ì´ë¯¸ ì²˜ë¦¬ëœ ê²½ìš° ìŠ¤í‚µ)
        # ë‹¤ìš´ë¡œë“œ ì‹œì‘ ì „ ìš©ëŸ‰ ì²´í¬ ë¡œì§ ìˆ˜í–‰
        if check_storage(key, download_dir=DOWNLOAD_DIR):
            download_success = download_file(key, download_dir=DOWNLOAD_DIR)
            if not download_success:
                pipeline_logger.warning(f"âŒ [ì—ëŸ¬] íŒŒì¼ {key} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨. í•´ë‹¹ ì²­í¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                continue
            
        # [Step 2] ì••ì¶• í•´ì œ (ìµœì‹  zip íŒŒì¼ ê¸°ì¤€)
        unzip_success = unzip_file(key, zip_dir=DOWNLOAD_DIR, extract_dir=EXTRACT_DIR)
        if not unzip_success:
            pipeline_logger.warning(f"âš ï¸ [ì˜¤ë¥˜] {key} ì••ì¶• í•´ì œ ì‹¤íŒ¨. í•´ë‹¹ ì²­í¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
            
        # [Step 3] ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (í•´ë‹¹ íŒŒì¼ ìºì‹œ ë””ë ‰í† ë¦¬ë§Œ ëŒ€ìƒ)
        current_src_dir = os.path.join(EXTRACT_DIR, str(key))
        current_dst_dir = os.path.join(TRANSFORM_DST_DIR, str(key))
        
        transform_success = run_transform_for_chunk(
            chunk_key=f"transform_{key}",
            src_root=current_src_dir,
            dst_root=current_dst_dir
        )
        if not transform_success:
            pipeline_logger.warning(f"âš ï¸ [ì˜¤ë¥˜] {key} ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨. ë‹¤ìŒ ì²­í¬ë¡œ.")
            continue
            
        # [Step 4] ì„ì‹œ ZIP ì••ì¶• (GCP ë¶„ë¦¬ ì €ì¥ ì „ ë‹¨ì¼ íŒŒì¼í™”)
        archive_file = compress_folder(
            chunk_key=f"archive_{key}",
            source_folder=current_dst_dir
        )
        if not archive_file:
            pipeline_logger.warning(f"âš ï¸ [ì˜¤ë¥˜] {key} íŒ¨í‚¤ì§•(ì••ì¶•) ì‹¤íŒ¨. ë‹¤ìŒ ì²­í¬ë¡œ.")
            continue
            
        # [Step 5] í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ ë‹¨ì¼ íŒŒì¼ ì ì¬ (GCP Cloud Storage)
        upload_success = upload_to_s3(
            chunk_key=f"upload_{key}",
            archive_file=archive_file,
            bucket_name=BUCKET_NAME
        )
        if not upload_success:
            pipeline_logger.warning(f"âš ï¸ [ì˜¤ë¥˜] {key} íŒ¨í‚¤ì§€(GCP) ì—…ë¡œë“œ ì‹¤íŒ¨.")
            continue
            
        # [Step 6] S3 ì—…ë¡œë“œê¹Œì§€ ì™„ë²½íˆ ëë‚œ ê²½ìš°ì— í•œí•´ ë¡œì»¬ ìŠ¤í…Œì´ì§• íŒŒì¼ í´ë¦°ì—…
        cleanup_chunk_files(file_key=key)
        
    pipeline_logger.info("\nğŸ‰ ì „ì²´ íŒŒì¼ ì²˜ë¦¬ ë° íŠ¸ëœì­ì…˜ íŒŒì´í”„ë¼ì¸ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!! ğŸ‰")

if __name__ == "__main__":
    run_pipeline()
