import cv2
import numpy as np
import os
import time
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor
from functools import partial
from tqdm import tqdm

from config.default import *
from logger import ChunkTracker, step_monitor, pipeline_logger

transform_tracker = ChunkTracker(state_file=LOG_DIR / "transform_state.json")

def get_dir_size_bytes(path):
    """í´ë”ì˜ ì „ì²´ ìš©ëŸ‰ì„ Byte ë‹¨ìœ„ë¡œ ê³„ì‚° (ì •ë°€ë„ ìœ ì§€)"""
    total = 0
    try:
        for entry in os.scandir(path):
            if entry.is_file():
                total += entry.stat().st_size
            elif entry.is_dir():
                total += get_dir_size_bytes(entry.path)
    except Exception:
        pass
    return total

def print_summary_report(start_time, end_time, processed_count, dst_root, skipped_count=None):
    duration = end_time - start_time
    final_size_bytes = get_dir_size_bytes(dst_root)
    final_size_gb = final_size_bytes / (1024 ** 3)
    final_size_mb = final_size_bytes / (1024 ** 2)
    avg_speed = processed_count / duration if duration > 0 else 0
    avg_size_kb = (final_size_bytes / processed_count) / 1024 if processed_count > 0 else 0

    print("\n" + "="*50)
    print("ğŸ“‹ [ì‘ì—… ì™„ë£Œ ìš”ì•½ ë¦¬í¬íŠ¸]")
    print("="*50)
    print(f"âœ… ì´ ì²˜ë¦¬ ì´ë¯¸ì§€: {processed_count:,} ì¥")
    if skipped_count is not None:
        print(f"â­ï¸ ìŠ¤í‚µ ì´ë¯¸ì§€: {skipped_count:,} ì¥")
    print(f"ğŸ“¦ ì „ì²´ ì €ì¥ ìš©ëŸ‰: {final_size_gb:.2f} GB ({final_size_mb:.2f} MB)")
    print(f"ğŸ–¼ï¸ ì¥ë‹¹ í‰ê·  ìš©ëŸ‰: {avg_size_kb:.2f} KB")
    print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„  : {duration/60:.1f} ë¶„")
    print(f"âš¡ í‰ê·  ì²˜ë¦¬ ì†ë„: {avg_speed:.2f} img/s")
    print("="*50)


def resize_with_padding(image_path: Path,
                        src_root: str = TRANSFORM_SRC_DIR,
                        dst_root: str = TRANSFORM_DST_DIR,
                        target_size: int = TARGET_SIZE):
    """ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì§• ë° ì €ì¥ í•µì‹¬ ë¡œì§"""
    try:
        img = cv2.imread(str(image_path))
        if img is None: return None
        
        h, w = img.shape[:2]
        ratio = target_size / max(h, w)
        new_h, new_w = int(h * ratio), int(w * ratio)
        resized = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)
        
        canvas = np.zeros((target_size, target_size, 3), dtype=np.uint8)
        canvas[(target_size-new_h)//2:(target_size-new_h)//2+new_h, 
               (target_size-new_w)//2:(target_size-new_w)//2+new_w] = resized
        
        relative_path = image_path.relative_to(Path(src_root))
        save_path = Path(dst_root) / relative_path.with_suffix('.webp')
        save_path.parent.mkdir(parents=True, exist_ok=True)
        
        cv2.imwrite(str(save_path), canvas, [cv2.IMWRITE_WEBP_QUALITY, 90])
        return relative_path
    except Exception:
        return None


@step_monitor(transform_tracker)
def run_transform_for_chunk(chunk_key, src_root: str = TRANSFORM_SRC_DIR, dst_root: str = TRANSFORM_DST_DIR):
    """ì§€ì •ëœ ì²­í¬(íŒŒì¼/í´ë” ë“±) ë‹¨ìœ„ë¡œ ì´ë¯¸ì§€ ë³€í™˜ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜.
       í˜„ì¬ëŠ” ì „ì²´ í´ë”ë¥¼ í•œ ë²ˆì— ë³€í™˜í•˜ë„ë¡ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë¯€ë¡œ chunk_key="all_images" í˜•íƒœë¡œ í˜¸ì¶œ ê°€ëŠ¥í•©ë‹ˆë‹¤."""
    
    pipeline_logger.info("ğŸš€ ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì§• ë³‘ë ¬ ì²˜ë¦¬ ì¤€ë¹„ ì¤‘...")
    
    # ì´ë¯¸ì§€ ì™¸ì˜ ë©”íƒ€ë°ì´í„°(json ë“±) íŒŒì¼ë„ ê²°ê³¼ í´ë”ë¡œ ë³µì‚¬í•˜ê¸° ìœ„í•´ íŒŒì¼ ëª©ë¡ ë¶„ë¥˜
    all_files = [f for f in Path(src_root).rglob('*') if f.is_file()]
    image_files = [f for f in all_files if f.suffix.lower() in ('.jpg', '.png', '.webp')]
    json_files = [f for f in all_files if f.suffix.lower() == '.json']
    
    total_images = len(image_files)
    
    start_time = time.time() # ì‹œì‘ ì‹œê°„ ê¸°ë¡
    
    # ğŸš€ JSON ë“± ë©”íƒ€ë°ì´í„° ë¼ë²¨ íŒŒì¼ ë³µì‚¬ ì²˜ë¦¬
    pipeline_logger.info(f"ğŸ“‚ ë¼ë²¨ë§ ë°ì´í„°(JSON) ë³µì‚¬ ì‹œì‘... (ì´ {len(json_files)}ê°œ)")
    import shutil
    for j_file in json_files:
        relative_path = j_file.relative_to(Path(src_root))
        save_path = Path(dst_root) / relative_path
        save_path.parent.mkdir(parents=True, exist_ok=True)
        shutil.copy2(str(j_file), str(save_path))
        
    pipeline_logger.info(f"âœ… ë¼ë²¨ë§ ë°ì´í„° ë³µì‚¬ ì™„ë£Œ!")
    
    # ğŸš€ tqdm ì§„í–‰ë°” ì‹œì‘
    pbar = tqdm(total=total_images, desc="ğŸš€ Resizing", unit="img", colour='green')
    actual_images = 0
    
    with ProcessPoolExecutor() as executor:
        func = partial(resize_with_padding, src_root=src_root, dst_root=dst_root)
        for res in executor.map(func, image_files):
            if res:
                pbar.set_postfix(class_name=res.parent.name)
                actual_images += 1
            pbar.update(1)
            
    pbar.close()
    end_time = time.time() # ì¢…ë£Œ ì‹œê°„ ê¸°ë¡

    # --- ğŸ“Š ìµœì¢… ë¦¬í¬íŠ¸ ê³„ì‚° ë° ì¶œë ¥ ---
    print_summary_report(start_time, end_time, actual_images, dst_root)
    target_success = (actual_images == total_images)
    return target_success
