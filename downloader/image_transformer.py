import cv2
import numpy as np
import os
import time
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor
from functools import partial

from config import *
from utils.logger import with_logging, pipeline_logger

def get_dir_size_bytes(path):
    """í´ë”ì˜ ì „ì²´ ìš©ëŸ‰ì„ Byte ë‹¨ìœ„ë¡œ ê³„ì‚° (ì •ë°€ë„ ìœ ì§€)"""
    total = 0
    try:
        for entry in os.scandir(path):
            if entry.is_file():
                total += entry.stat().st_size
            elif entry.is_dir():
                total += get_dir_size_bytes(entry.path)
    except Exception:
        pass
    return total

def print_summary_report(start_time, end_time, processed_count, dst_root, skipped_count=None):
    duration = end_time - start_time
    final_size_bytes = get_dir_size_bytes(dst_root)
    final_size_gb = final_size_bytes / (1024 ** 3)
    final_size_mb = final_size_bytes / (1024 ** 2)
    avg_speed = processed_count / duration if duration > 0 else 0
    avg_size_kb = (final_size_bytes / processed_count) / 1024 if processed_count > 0 else 0

    print("\n" + "="*50)
    print("ğŸ“‹ [ì‘ì—… ì™„ë£Œ ìš”ì•½ ë¦¬í¬íŠ¸]")
    print("="*50)
    print(f"âœ… ì´ ì²˜ë¦¬ ì´ë¯¸ì§€: {processed_count:,} ì¥")
    if skipped_count is not None:
        print(f"â­ï¸ ìŠ¤í‚µ ì´ë¯¸ì§€: {skipped_count:,} ì¥")
    print(f"ğŸ“¦ ì „ì²´ ì €ì¥ ìš©ëŸ‰: {final_size_gb:.2f} GB ({final_size_mb:.2f} MB)")
    print(f"ğŸ–¼ï¸ ì¥ë‹¹ í‰ê·  ìš©ëŸ‰: {avg_size_kb:.2f} KB")
    print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„  : {duration/60:.1f} ë¶„")
    print(f"âš¡ í‰ê·  ì²˜ë¦¬ ì†ë„: {avg_speed:.2f} img/s")
    print("="*50)


def resize_with_padding(image_path: Path,
                        src_root: Path = TRANSFORM_SRC_DIR,
                        dst_root: Path = TRANSFORM_DST_DIR,
                        target_size: int = TARGET_SIZE):
    """ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì§• ë° ì €ì¥ í•µì‹¬ ë¡œì§"""
    img = cv2.imread(str(image_path))
    if img is None:
        return None
    h, w = img.shape[:2]
    ratio = target_size / max(h, w)
    new_h, new_w = int(h * ratio), int(w * ratio)
    resized = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)
    
    canvas = np.zeros((target_size, target_size, 3), dtype=np.uint8)
    canvas[(target_size-new_h)//2:(target_size-new_h)//2+new_h, 
            (target_size-new_w)//2:(target_size-new_w)//2+new_w] = resized
    
    relative_path = image_path.relative_to(src_root)
    save_path = dst_root / relative_path.with_suffix('.webp')
    save_path.parent.mkdir(parents=True, exist_ok=True)
    
    cv2.imwrite(str(save_path), canvas, [cv2.IMWRITE_WEBP_QUALITY, 90])
    return relative_path

@with_logging
def run_transform_for_chunk(chunk_key, src_root: Path = TRANSFORM_SRC_DIR, dst_root: Path = TRANSFORM_DST_DIR):
    """ì§€ì •ëœ ì²­í¬(íŒŒì¼/í´ë” ë“±) ë‹¨ìœ„ë¡œ ì´ë¯¸ì§€ ë³€í™˜ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜.
       í˜„ì¬ëŠ” ì „ì²´ í´ë”ë¥¼ í•œ ë²ˆì— ë³€í™˜í•˜ë„ë¡ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë¯€ë¡œ chunk_key="all_images" í˜•íƒœë¡œ í˜¸ì¶œ ê°€ëŠ¥í•©ë‹ˆë‹¤."""
    
    # ğŸš€ JSON ë° ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    src_path = Path(src_root)
    dst_path = Path(dst_root)
    
    # rglobì„ ì‚¬ìš©í•˜ì—¬ í•˜ìœ„ í´ë”ê¹Œì§€ ëª¨ë“  ì´ë¯¸ì§€ì™€ JSON íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤.
    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.webp'}
    image_files = [f for f in src_path.rglob('*') if f.suffix.lower() in image_extensions]
    json_files = list(src_path.rglob('*.json'))
    total_images = len(image_files)
    
    # ğŸš€ JSON ë“± ë©”íƒ€ë°ì´í„° ë¼ë²¨ íŒŒì¼ ë³µì‚¬ ì²˜ë¦¬
    import shutil
    for j_file in json_files:
        relative_path = j_file.relative_to(src_path)
        save_path = dst_path / relative_path
        save_path.parent.mkdir(parents=True, exist_ok=True)
        shutil.copy2(str(j_file), str(save_path))
            
    actual_images = 0
    start_time = time.time()
    
    # =========================================================================
    # âš™ï¸ ProcessPoolExecutor ê°€ì´ë“œ (ë©€í‹°í”„ë¡œì„¸ì‹±)
    # =========================================================================
    # 1. ëª©ì : CPU ì§‘ì•½ì ì¸ ì‘ì—…(ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì§•, ì—°ì‚° ë“±)ì„ ì—¬ëŸ¬ CPU ì½”ì–´ì— ë¶„ì‚°ì‹œì¼œ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    # 2. ë™ì‘ ë°©ì‹:
    #    - ê° ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ ë³„ë„ì˜ 'í”„ë¡œì„¸ìŠ¤'ë¡œ ìƒì„±í•˜ì—¬ ë…ë¦½ëœ ë©”ëª¨ë¦¬ ê³µê°„ì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤.
    #    - íŒŒì´ì¬ì˜ GIL(Global Interpreter Lock)ì„ ìš°íšŒí•˜ì—¬ ì‹¤ì§ˆì ì¸ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ê°€ëŠ¥ì¼€ í•©ë‹ˆë‹¤.
    # 3. ì£¼ìš” êµ¬ì„± ìš”ì†Œ:
    #    - max_workers: ë™ì‹œì— ì‹¤í–‰í•  í”„ë¡œì„¸ìŠ¤ ìˆ˜ (ê¸°ë³¸ê°’ì€ CPU ì½”ì–´ ìˆ˜).
    #    - executor.map(func, iterable): func í•¨ìˆ˜ì— iterableì˜ ê° ìš”ì†Œë¥¼ í•˜ë‚˜ì”© ì „ë‹¬í•˜ë©° ë³‘ë ¬ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.
    #    - partial: ê³ ì •ëœ ì¸ì(src_root, dst_root ë“±)ë¥¼ ë¯¸ë¦¬ ì±„ì›Œ ë„£ì€ ìƒˆë¡œìš´ í•¨ìˆ˜ë¥¼ ë§Œë“œëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
    # =========================================================================
    
    with ProcessPoolExecutor() as executor:
        # ê³ ì • ì¸ìë¥¼ ë¯¸ë¦¬ ì„¤ì •í•œ partial í•¨ìˆ˜ ìƒì„±
        func = partial(resize_with_padding, src_root=src_path, dst_root=dst_path)
        
        # ë³‘ë ¬ ì‹¤í–‰ ì‹œì‘
        for res in executor.map(func, image_files):
            if res:
                actual_images += 1
                
    end_time = time.time() # ì¢…ë£Œ ì‹œê°„ ê¸°ë¡

    # --- ğŸ“Š ìµœì¢… ë¦¬í¬íŠ¸ ê³„ì‚° ë° ì¶œë ¥ ---
    print_summary_report(start_time, end_time, actual_images, dst_path)
    target_success = (actual_images == total_images)
    return target_success
